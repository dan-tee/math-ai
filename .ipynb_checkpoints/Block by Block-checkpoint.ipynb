{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb36e15-e88c-475c-915e-f82609610bf7",
   "metadata": {},
   "source": [
    "# Block by Block - DeepSeek\n",
    "- vLLM in fp16\n",
    "\n",
    "\n",
    "### TODO:\n",
    "1. Use string output in case there is no code output\n",
    "2. Feed error message into code retries\n",
    "3. Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681bc6fa-9425-445f-91b9-eb49b9363ce5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory (GB, approx.): 23.64\n",
      "WARNING 04-30 17:28:05 config.py:767] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-30 17:28:05 config.py:381] Using fp8_e5m2 data type to store kv cache. It reduces the GPU memory footprint and boosts the performance. But it may cause slight accuracy drop. Currently we only support fp8 without scaling factors and make e5m2 as a default format.\n",
      "INFO 04-30 17:28:05 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='deepseek-ai/deepseek-math-7b-rl', tokenizer='deepseek-ai/deepseek-math-7b-rl', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=True, kv_cache_dtype=fp8_e5m2, device_config=cuda, seed=0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-30 17:28:06 selector.py:51] Cannot use FlashAttention because the package is not found. Please install it for better performance.\n",
      "INFO 04-30 17:28:06 selector.py:25] Using XFormers backend.\n",
      "INFO 04-30 17:28:07 weight_utils.py:177] Using model weights format ['*.safetensors']\n",
      "INFO 04-30 17:28:10 model_runner.py:104] Loading model weights took 12.8725 GB\n",
      "INFO 04-30 17:28:10 gpu_executor.py:94] # GPU blocks: 2569, # CPU blocks: 1092\n"
     ]
    }
   ],
   "source": [
    "import os, io\n",
    "import json\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import re, sys, subprocess, gc\n",
    "import time\n",
    "import multiprocessing\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from multiprocessing import Pool\n",
    "\n",
    "n_questions = 1000\n",
    "n_reps_per_prompt = 3\n",
    "n_code_retries = 4\n",
    "code_timeout_secs = 2\n",
    "temperature = 0.9\n",
    "code_temperature = 0.3\n",
    "n_code_processes = 8\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def free_mem():\n",
    "    if hasattr(sys, 'last_traceback'):\n",
    "        sys.last_traceback.tb_next = None\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def print_cuda_mem():\n",
    "    total_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
    "    cached_memory = torch.cuda.memory_reserved(0) / (1024 ** 3)\n",
    "    available_memory = total_memory - cached_memory \n",
    "    print(f\"Available Memory (GB, approx.): {available_memory:.2f}\")\n",
    "\n",
    "ds_math_rl = \"deepseek-ai/deepseek-math-7b-rl\"\n",
    "#\"mixtral Â· 8x7b-instruct-v0.1-hf\" ???\n",
    "\n",
    "\n",
    "torch_dtype = torch.bfloat16\n",
    "# to avoid warning when spawing processes to evaluate code later\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "print_cuda_mem()\n",
    "\n",
    "if not 'vllm' in globals():\n",
    "    vllm = LLM(model=ds_math_rl,\n",
    "               dtype='half',\n",
    "               enforce_eager=True,\n",
    "               gpu_memory_utilization=0.99,\n",
    "               swap_space=4,\n",
    "               max_model_len=2048,\n",
    "               kv_cache_dtype=\"fp8_e5m2\",\n",
    "               tensor_parallel_size=1)\n",
    "vtokenizer = vllm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ffba91-88ab-40af-ba46-31db6ac025aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let $k, l > 0$ be parameters. The parabola $y = kx^2 - 2kx + l$ intersects the line $y = 4$ at two points $A$ and $B$. These points are distance 6 apart. What is the sum of the squares of the distances from $A$ and $B$ to the origin?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>problem</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229ee8</td>\n",
       "      <td>Let $k, l &gt; 0$ be parameters. The parabola $y ...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>246d26</td>\n",
       "      <td>Each of the three-digits numbers $111$ to $999...</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2fc4ad</td>\n",
       "      <td>Let the `sparkle' operation on positive intege...</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                            problem  answer\n",
       "0  229ee8  Let $k, l > 0$ be parameters. The parabola $y ...      52\n",
       "1  246d26  Each of the three-digits numbers $111$ to $999...     250\n",
       "2  2fc4ad  Let the `sparkle' operation on positive intege...     702"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_df = pd.read_csv('./train.csv')\n",
    "print(q_df.iloc[0]['problem'])\n",
    "q_df.iloc[0]['answer']\n",
    "q_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23aca986-4919-4507-8a65-44c730a8e833",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4748, 6) (3119, 6)\n"
     ]
    }
   ],
   "source": [
    "from MATH.math_equivalence import is_equiv\n",
    "def extract_answer(text, boxed_in_tool_instruction=True):\n",
    "    pattern = r\"\\\\boxed\\{([^}]*)\\}\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    n_matches = 2 if boxed_in_tool_instruction else 1\n",
    "    \n",
    "    if len(matches) >= n_matches:\n",
    "        try:\n",
    "            return int(matches[-1]) % 1000\n",
    "        except ValueError:\n",
    "            return None\n",
    "\n",
    "    numbers = re.findall(r'[+-]?\\d+', text)\n",
    "    return int(numbers[-1]) % 1000 if numbers else -1\n",
    "\n",
    "def extract_correct(text):\n",
    "    return extract_answer(text, False)\n",
    "\n",
    "def read_math_data(folder_path, train_test):\n",
    "    folder = Path(folder_path)\n",
    "    math_data = []\n",
    "    for sub_folder in os.listdir(folder):\n",
    "        if not os.path.isdir(folder/sub_folder):\n",
    "            continue\n",
    "        for file_name in os.listdir(folder/sub_folder):\n",
    "            if file_name.endswith('.json'):\n",
    "                with open(folder / sub_folder / file_name, 'r') as file:\n",
    "                    data = json.load(file)\n",
    "                    data['id'] = train_test + '-' + sub_folder + '-' + file_name[:-5]\n",
    "                    math_data.append(data)\n",
    "    return pd.DataFrame(math_data)\n",
    "\n",
    "math_train = read_math_data('./MATH/train', 'train')\n",
    "math_train['answer'] = math_train['solution'].map(extract_correct)\n",
    "math_train = math_train[math_train['answer'].notnull()]\n",
    "math_train['answer'] = math_train['answer'].astype(int)\n",
    "math_test = read_math_data('./MATH/test', 'test')\n",
    "math_test['answer'] = math_test['solution'].map(extract_correct)\n",
    "math_test = math_test[math_test['answer'].notnull()]\n",
    "math_test['answer'] = math_test['answer'].astype(int)\n",
    "print(math_train.shape, math_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec196a18-24b6-4383-9a1f-7b345ade501b",
   "metadata": {},
   "source": [
    "- 1 yes\n",
    "- 2 no\n",
    "- 3 no\n",
    "- 4 yes\n",
    "- 5 maybe\n",
    "- 6 maybe\n",
    "- 7 yes\n",
    "- 8 maybe\n",
    "- 9 yes\n",
    "- 10 no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1089cd-82bd-41f6-aed1-d04f2a343063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['problem', 'level', 'type', 'solution', 'id', 'answer'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_samp = math_test[math_test['level'].isin(['Level 1', 'Level 2', 'Level 3', 'Level 4', 'Level 5'])].sample(n_questions)\n",
    "math_samp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc5cafd3-df78-4efd-a2bd-1e960386fb90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem</th>\n",
       "      <th>level</th>\n",
       "      <th>type</th>\n",
       "      <th>solution</th>\n",
       "      <th>id</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2857</th>\n",
       "      <td>Given $2^a = 32$ and $a^b = 125$ find $b^a$.</td>\n",
       "      <td>Level 1</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>We note that $32 = 2 \\cdot 2\\cdot 2\\cdot 2\\cdo...</td>\n",
       "      <td>test-algebra-756</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3540</th>\n",
       "      <td>How many distinct three-letter sequences with ...</td>\n",
       "      <td>Level 5</td>\n",
       "      <td>Counting &amp; Probability</td>\n",
       "      <td>We solve by casework.\\n\\n$\\bullet$ Case I: Exa...</td>\n",
       "      <td>test-counting_and_probability-381</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3975</th>\n",
       "      <td>Compute $\\cos 180^\\circ$.</td>\n",
       "      <td>Level 1</td>\n",
       "      <td>Precalculus</td>\n",
       "      <td>Rotating the point $(1,0)$ about the origin by...</td>\n",
       "      <td>test-precalculus-1282</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                problem    level  \\\n",
       "2857       Given $2^a = 32$ and $a^b = 125$ find $b^a$.  Level 1   \n",
       "3540  How many distinct three-letter sequences with ...  Level 5   \n",
       "3975                          Compute $\\cos 180^\\circ$.  Level 1   \n",
       "\n",
       "                        type  \\\n",
       "2857                 Algebra   \n",
       "3540  Counting & Probability   \n",
       "3975             Precalculus   \n",
       "\n",
       "                                               solution  \\\n",
       "2857  We note that $32 = 2 \\cdot 2\\cdot 2\\cdot 2\\cdo...   \n",
       "3540  We solve by casework.\\n\\n$\\bullet$ Case I: Exa...   \n",
       "3975  Rotating the point $(1,0)$ about the origin by...   \n",
       "\n",
       "                                     id  answer  \n",
       "2857                   test-algebra-756     243  \n",
       "3540  test-counting_and_probability-381      48  \n",
       "3975              test-precalculus-1282     999  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with pd.option_context('display.max_rows', None):\n",
    "display(math_samp[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9622bee-4a97-4f2c-ba83-292e0552840b",
   "metadata": {},
   "source": [
    "# Batched Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dddc6a4-1c58-49df-9ef7-d7970cdbcf89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "short_instruction = \"\\nPlease reason step by step and use Python code. The final answer is an integer and should be placed within \\\\boxed{}.\"\n",
    "short_code_instruction = \"\\nDescribe a high level strategy to solve the problem. Output Python with SymPy code to solve the problem. The final answer is an integer and should be placed within \\\\boxed{}.\"\n",
    "derive_instruction_long = \"\\nFirst translate the problem into equations and try to derive useful other equations.\\n\" \\\n",
    "                          + \"Then use Python code to solve the problem. The final answer is an integer and should be placed within \\\\boxed{}.\"\n",
    "early_win_instruction = \"\"\"\\nTo solve the problem first determine a sympy-based approach for solving the problem by listing each step to take and what functions need to be called in each step.\\n\n",
    "Write a Python script covering all the steps and print the result. The final answer is an integer and should be placed within \\\\boxed{}.\n",
    "\n",
    "Approach:\"\"\"\n",
    "\n",
    "prompt_variations = {\n",
    "    'short_instruction' : short_instruction,\n",
    "    'short_code_instruction' : short_code_instruction,\n",
    "    'derive_instruction_long': derive_instruction_long,\n",
    "    'early_win_instruction': early_win_instruction\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197315b8-34ae-4427-acca-7bcb0b007f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   6%|â         | 695/12000 [01:31<15:33, 12.11it/s]  "
     ]
    }
   ],
   "source": [
    "batched_prompts = []\n",
    "out_rows = []\n",
    "for rep in range(n_reps_per_prompt):\n",
    "    for inst_idx, inst_key_val in enumerate(prompt_variations.items()):\n",
    "        for i, problem in enumerate(math_samp['problem']):\n",
    "            prompt = problem + inst_key_val[1]\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            m_prompt = vtokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "            batched_prompts.append(m_prompt)\n",
    "            out_rows.append({\n",
    "                'problem_id': math_samp.iloc[i]['id'],\n",
    "                'problem': math_samp.iloc[i]['problem'],\n",
    "                'correct_answer': math_samp.iloc[i]['answer'],\n",
    "                'iteration': 0,\n",
    "                'rep': rep * len(prompt_variations) + inst_idx,\n",
    "                'level': math_samp.iloc[i]['level'],\n",
    "                'type': math_samp.iloc[i]['type'],\n",
    "                'prompt_variation': inst_key_val[0],\n",
    "                'raw_prompt': prompt,\n",
    "                'chat_prompt': m_prompt\n",
    "            })\n",
    "    \n",
    "tool_sampling_params = SamplingParams(n=1,\n",
    "                                      temperature=temperature,\n",
    "                                      max_tokens=1400,\n",
    "                                      stop='python',\n",
    "                                      include_stop_str_in_output=True)\n",
    "before_code_out = vllm.generate(batched_prompts, tool_sampling_params)\n",
    "\n",
    "n_out_tokens = 0\n",
    "for i, output in enumerate(before_code_out):\n",
    "    out_rows[i]['out_before_code'] = output.outputs[0].text\n",
    "    out_rows[i]['n_before_code_tokens'] = len(output.outputs[0].token_ids)\n",
    "\n",
    "df_res = pd.DataFrame(out_rows)\n",
    "df_res[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7cbe5b-74ef-471d-91dc-ae3313e7efda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_= df_res['n_before_code_tokens'].hist(figsize=(6,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b00550-1b4b-4c16-bf80-59d28482d83b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### !!! Most answers before the code already tried to completely answer the question.\n",
    "\n",
    "### ToDo: Try out a code focussed instruction for code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d44a85-1e79-4a24-8dca-ccf240c76d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad256c0-af91-47f3-9605-f62c21c4e081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i += 1\n",
    "df_res['level'].iloc[i], df_res['problem'].iloc[i], df_res['correct_answer'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb353e-8af0-4683-bfe9-a923ef96299d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res['out_before_code'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd9accf-2809-4795-822b-dabe6d5b76e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res['code_prompt'] = df_res['chat_prompt'] + df_res['out_before_code']\n",
    "python_rows = df_res['code_prompt'].str.endswith('python')\n",
    "df_res.loc[~python_rows, 'code_prompt'] += \"\\n```python\\n\"\n",
    "\n",
    "code_sampling_params = SamplingParams(n=1,\n",
    "                                      temperature=code_temperature,\n",
    "                                      max_tokens=1900,\n",
    "                                      stop='```')\n",
    "code_out = vllm.generate(df_res['code_prompt'], code_sampling_params)\n",
    "\n",
    "code_results = []\n",
    "n_code_tokens = []\n",
    "for output in code_out:\n",
    "    code_results.append(output.outputs[0].text)\n",
    "    n_code_tokens.append(len(output.outputs[0].token_ids))\n",
    "df_res['code'] = code_results\n",
    "df_res['n_code_tokens'] = n_code_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca462f3-8326-479b-827e-ecc5b9f69e52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_= df_res['n_code_tokens'].hist(figsize=(6,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540c872-9936-4b72-b979-1baa4d505074",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_code(output):\n",
    "    if '```python\\n' in output:\n",
    "        try:\n",
    "            output = output.split(python_code_prefix)[-1]\n",
    "        except:\n",
    "            return None   \n",
    "    if '```' in output:\n",
    "        output = output.split('```')[0]\n",
    "    #output = python_default_imports + output\n",
    "    return add_workarounds(output)\n",
    "\n",
    "def add_workarounds(code):\n",
    "    import re\n",
    "    def repl(match):\n",
    "        if \"real\" not in match.group():\n",
    "            return \"{}{}\".format(match.group()[:-1], ', real=True)')\n",
    "        else:\n",
    "            return \"{}{}\".format(match.group()[:-1], ')')\n",
    "    code = re.sub(r\"symbols\\([^)]+\\)\", repl, code)\n",
    "    \n",
    "    pattern = r\"\\s(?:cbrt|root)\\((.*?)(?:, 3)?\\)\"\n",
    "    replacement = r\"real_root(\\1, 3)\"\n",
    "    code = re.sub(pattern, replacement, code)\n",
    "    code = re.sub(r'permutations\\(', 'itertools.permutations(', code)\n",
    "    \n",
    "    code = 'from sympy import *\\nimport math, itertools\\n' + code\n",
    "    if 'np.' in code:\n",
    "        code = 'import numpy as np\\n' + code\n",
    "    return code\n",
    "\n",
    "def run_with_timeout(code, timeout):\n",
    "    #ToDo: check whether sometimes there is a print of the corect result before an exception\n",
    "    \n",
    "    unique_filename = f'code_{uuid.uuid4().hex}.py'\n",
    "    with open(unique_filename, 'w') as fout:\n",
    "        fout.write(code)\n",
    "\n",
    "    batcmd = f'timeout {timeout} {sys.executable} {unique_filename}'\n",
    "    try:\n",
    "        shell_output = subprocess.check_output(batcmd, stderr=subprocess.STDOUT, shell=True).decode('utf8')\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        shell_output = e.output.decode('utf8')  # Get the output which may include Python error messages\n",
    "    try:\n",
    "        out_eval = eval(shell_output.strip())\n",
    "        if type(out_eval) == list:\n",
    "            out_eval = out_eval[0]\n",
    "        code_output = round(float(out_eval)) % 1000  # <----------   !!!!!!!!!!!!   modulo important for competition\n",
    "    except Exception as e:\n",
    "        code_output = shell_output\n",
    "    os.remove(unique_filename)\n",
    "    return code_output\n",
    "\n",
    "def parse_and_run(code):\n",
    "    try:\n",
    "        code = parse_code(code)\n",
    "    except Exception as ex:\n",
    "        return str(ex)\n",
    "    try:\n",
    "        return run_with_timeout(code, code_timeout_secs)\n",
    "    except Exception as ex:\n",
    "        return str(ex)\n",
    "    \n",
    "def run_code_parallel(code_series):\n",
    "    with Pool(processes=n_code_processes) as pool:\n",
    "        return pool.map(parse_and_run, [item for item in code_series])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51d2160-31a9-40d4-8957-20fa62422b37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res['parsed_code'] = df_res['code'].map(parse_code)\n",
    "start_time = time.time()\n",
    "df_res['code_result'] = run_code_parallel(df_res['code'])\n",
    "print(f\"Ran code outputs in {round(time.time() - start_time)}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c7b44d-662e-41db-a5ee-88914bcc88b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res[df_res['code_result'].str.contains('.py').fillna(False)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada0406-19ca-489d-a745-18fdfd6982a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res['code_result'].str.contains('Traceback').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8c8d9d-1ec2-474d-8c52-f9f5ca62658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res['int_code_result'] = pd.to_numeric(df_res['code_result'], errors='coerce')\n",
    "df_res['int_code_result'] = df_res['int_code_result'].fillna(-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50d828-a8c9-4c4b-8bc3-289f2e77d1aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res['code_prompt'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3e85a-73a7-4c10-bb71-ae3895923a01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323cb30-34a5-4746-8179-f66d61a029ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "increasing_code_temperature = code_temperature\n",
    "\n",
    "for i in range(n_code_retries):\n",
    "    if increasing_code_temperature <= 0.8:\n",
    "        increasing_code_temperature += 0.2\n",
    "    else:\n",
    "        increasing_code_temperature = 1\n",
    "      \n",
    "    missing = df_res[df_res['int_code_result'] == -1]\n",
    "    print(f\"{len(missing)} missing or non-numerical code results\")\n",
    "    last_error_lines = missing['code_result'].str.split('\\n').apply(lambda x: '\\n'.join(x[-10:]))\n",
    "    new_user_prompts = missing['raw_prompt'] + '\\nTry to avoid this error from the previous attempt: \\n\\n' + last_error_lines \n",
    "    m_prompts = []\n",
    "    for prompt in new_user_prompts:\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        m_prompt = vtokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "        m_prompts.append(m_prompt)\n",
    "    \n",
    "    missing['code_prompt'] = np.array(new_user_prompt) + missing['out_before_code']\n",
    "                            \n",
    "    if len(missing['code_prompt']) > 0:\n",
    "        print(missing['code_prompt'].iloc[0])\n",
    "    code_out = vllm.generate(missing['code_prompt'].values, code_sampling_params)\n",
    "\n",
    "    n_out_tokens = 0\n",
    "    code_results = []\n",
    "    n_code_tokens = []\n",
    "    for output in code_out:\n",
    "        code_results.append(output.outputs[0].text)\n",
    "        n_code_tokens.append(len(output.outputs[0].token_ids))\n",
    "    df_res.loc[missing.index, 'code'] = code_results\n",
    "    df_res.loc[missing.index, 'n_code_tokens'] = n_code_tokens\n",
    "\n",
    "    start_time = time.time()\n",
    "    df_res.loc[missing.index, 'code_result'] = run_code_parallel(df_res.loc[missing.index, 'code'])\n",
    "    print(f\"Ran code outputs in {round(time.time() - start_time)}s\")\n",
    "    \n",
    "    df_res.loc[missing.index, 'int_code_result'] = pd.to_numeric(df_res.loc[missing.index ,'code_result'], errors='coerce')\n",
    "    df_res['int_code_result'] = df_res['int_code_result'].fillna(-1).astype(int)\n",
    "missing = df_res[df_res['int_code_result'] == -1]\n",
    "print(f\"{len(missing)} missing or non-numerical code results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05508a-1c4f-4bdb-945b-54fc96250826",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing['code_prompt'].str.len().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb05da5-db6d-4e18-bd5c-8aa49bb77f37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_error_lines.str.len().hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff91346-7c0d-4ab2-884f-c3276de83fbb",
   "metadata": {},
   "source": [
    "### Inspect code errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae501ee-d395-474f-b6f6-72c7a312722c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missing['code_prompt'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f219be-ca70-498e-9e95-b6fcf7b5fc78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows = df_res['int_code_result'] == -1\n",
    "i = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2808079-7f1f-4c55-bb06-ed7db6e3fc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "i += 1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d79a166-e2c6-46cc-bac6-d0481be4a638",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df_res[rows]['problem'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f2f77b-eadd-4317-b48c-cd6e826bd980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df_res[rows]['parsed_code'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bcf968-e42e-4d12-8ef1-3d2e21b7799a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(run_with_timeout(df_res[rows]['parsed_code'].iloc[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b9539b-4eaa-492e-8847-e1f952bfdf2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(run_with_timeout(df_res[rows]['parsed_code'].iloc[i], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df5f67-1a76-4044-8ae1-ce13cb9a57b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c0659-dbe1-4f27-9e29-8d6d3a9407b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res['code_result'].astype(str).str.slice(0, 10).values[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d383b5-5638-4df3-9720-712dc83afe31",
   "metadata": {},
   "source": [
    "## Use output to generate answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5839333-80dd-4999-b799-c15432cf3c98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res['continue_prompt'] = df_res['code_prompt'] + df_res['code'] +'```\\n```output\\n' + df_res['code_result'].astype(str).str.slice(0, 10) + '\\n```\\n'\n",
    "\n",
    "tool_sampling_params = SamplingParams(n=1,\n",
    "                                      temperature=temperature,\n",
    "                                      max_tokens=2048)\n",
    "llm_out = vllm.generate(df_res['continue_prompt'], tool_sampling_params)\n",
    "\n",
    "continuations = []\n",
    "n_continue_tokens = []\n",
    "for llm_output in llm_out:\n",
    "    continuations.append(llm_output.outputs[0].text)\n",
    "    n_continue_tokens.append(len(llm_output.outputs[0].token_ids))\n",
    "    \n",
    "df_res['continuations'] = continuations\n",
    "df_res['n_continue_tokens'] = n_continue_tokens\n",
    "df_res['combined'] = df_res['continue_prompt'] + df_res['continuations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490736f6-186f-4064-9fb2-f39ff7f09459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# file_name = 'block_by_block_results_april_29.csv'\n",
    "# df_res.to_csv(file_name, index=False)\n",
    "# df_res = pd.read_csv(file_name)\n",
    "# df_res['problem_id'] = df_res['problem_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb87062-ccf9-45d0-ac70-ffe353e882db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def standardize_number(number):\n",
    "    try:\n",
    "        return int(number)\n",
    "    except:pass\n",
    "    try:\n",
    "        return float(number)\n",
    "    except: pass\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8cfcb0-a2f5-4f10-a059-1593770147f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res['llm_result'] = df_res['combined'].map(extract_answer)\n",
    "df_res['combined_result'] = df_res['code_result'].fillna(df_res['llm_result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e1d32-1621-4a98-b9f1-8d2e2fd49262",
   "metadata": {},
   "source": [
    "## Count Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa80ca-2bb1-4745-a0cc-70737a054e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def first_mode(series):\n",
    "    return series.mode().iloc[0] if not series.mode().empty else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62b45e-4109-4a07-9195-e72878e44b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = df_res.groupby('problem_id')['llm_result'].agg(first_mode)\n",
    "most_common = most_common.apply(lambda x: x[0] if isinstance(x, np.ndarray) and len(x) >= 1 else x)\n",
    "most_common = pd.DataFrame(most_common).reset_index()\n",
    "pooled = math_samp.merge(most_common, left_on='id', right_on='problem_id')\n",
    "is_correct = []\n",
    "for row in pooled.iterrows():\n",
    "    correct = standardize_number(row[1]['llm_result'])\n",
    "    result = standardize_number(row[1]['answer'])\n",
    "    is_correct.append(is_equiv(correct, result))\n",
    "pooled['is_correct'] = is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c178b1e-636e-440c-85cb-13714554100e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logistic_mean_and_cis(binary_series):\n",
    "    ci_lower, ci_upper = sm.stats.proportion_confint(count=binary_series.sum(), nobs=len(binary_series), alpha=0.05, method='agresti_coull')\n",
    "    return f\"{round(binary_series.mean(),2)} with CI  {round(ci_lower,2)}-{round(ci_upper,2)}\"\n",
    "    \n",
    "logistic_mean_and_cis(pooled['is_correct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177d85e6-7646-47ac-acc5-e7326f2521c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pooled.groupby('level')['is_correct'].agg(logistic_mean_and_cis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbceb72-4fc2-4126-86dc-888241733af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "most_common = df_res.groupby(['problem_id', 'prompt_variation'])['llm_result'].agg(first_mode)\n",
    "most_common = most_common.apply(lambda x: x[0] if isinstance(x, np.ndarray) and len(x) >= 1 else x)\n",
    "most_common = pd.DataFrame(most_common).reset_index()\n",
    "most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5dd032-5663-4111-8733-909726cefa3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_grouped = math_samp.merge(most_common, left_on='id', right_on='problem_id')\n",
    "is_correct = []\n",
    "for row in prompt_grouped.iterrows():\n",
    "    correct = standardize_number(row[1]['llm_result'])\n",
    "    result = standardize_number(row[1]['answer'])\n",
    "    is_correct.append(is_equiv(correct, result))\n",
    "prompt_grouped['is_correct'] = is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d978f-762c-4096-b02c-e95f3137588f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcabb94-afa3-410b-8c6c-13620f7d0d0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.barplot(prompt_grouped, x='prompt_variation', y='is_correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9001cd-4c13-4ad8-b90a-f73253f822db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_grouped['is_correct'].mean())\n",
    "prompt_grouped.groupby('prompt_variation')['is_correct'].agg(['size', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044afa7e-524f-4c49-a748-28b4dc776987",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_grouped.groupby('prompt_variation')['is_correct'].agg(['size', 'mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab57051-fefb-4a55-84df-e62fafe22fda",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "0.692\n",
    "size\tmean\n",
    "level\t\t\n",
    "Level 1\t23\t0.826087\n",
    "Level 2\t49\t0.816327\n",
    "Level 3\t66\t0.787879\n",
    "Level 4\t68\t0.647059\n",
    "Level 5\t44\t0.409091\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810869b9-d622-4a10-8d45-445be9c35d28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_res.to_csv('inject_code_results_april_23_2000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa40971-2244-4234-83e3-3e8620dc7205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pooled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5588c-2726-49f1-bfb3-38324ceefcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    display(pooled.loc[(pooled['level'] == 'Level 1') & (~pooled['is_correct']), ['id', 'level', 'type', 'answer', 'llm_result', 'is_correct']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8974af24-68f0-4eef-a0d9-9c454225313b",
   "metadata": {},
   "source": [
    "## Inspection of Errors\n",
    "- Problem 1016: Incorrect minimization of A \\cup B via overlaying A and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ca691-af95-459e-82a7-2b7f3ffd8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res[df_res['problem_id'] == '808']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95e75c-d528-4ee7-9dc2-a6ab36794b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea492a-849c-42b2-a193-f5d8f0e65062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0894fa9-dc8c-45dd-a42f-404d95681bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a60795-8c70-4a3e-b5a3-21ee53cbb1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838e944-e0e2-41e6-a0be-2eefc2bf164a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b231bbe2-03b9-4a55-b410-52dc1e079b15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a490d-dada-4b2f-b98d-c2d1cbfce57a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
